{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQnTd9rnpDWCNM7LLM3gdm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimeonHristov99/ML_21-22/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification. KNN and Logistic Regression"
      ],
      "metadata": {
        "id": "WWeJvtuHyftk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|             |continuous           | categorical    |\n",
        "|-----------  | --------------      | ----------     |\n",
        "|**supervised**   | regression          | **classification** |\n",
        "|**unsupervised** | dimension reduction | clustering     |"
      ],
      "metadata": {
        "id": "UKNBGLVVA_Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing to say is that logistic regression is not a regression, but a classification learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. In fact the only difference is that the polynomial gets passed through the so-called `sigmoid` function:\n",
        "\n",
        "$$result = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}}$$\n",
        "\n",
        "Note that:\n",
        "\n",
        "$$\\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x}$$"
      ],
      "metadata": {
        "id": "KQtJkhMD_lsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "be9KANYLB_8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIG_SIZE = (12, 10)\n",
        "plt.rc('figure', figsize=FIG_SIZE)"
      ],
      "metadata": {
        "id": "2Jf_NareCEVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  # return np.exp(x) / (1 + np.exp(x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "interval = np.linspace(-10, 10, num=1000)\n",
        "\n",
        "plt.plot(interval, sigmoid(interval))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iG_5G_xt0_X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sigmoid takes the value of 0.5 at 0 and goes towards 1 when moving towards $+\\infty$ and to 0 when moving towards $-\\infty$.\n",
        "\n",
        "We can use those two limits as the identifiers of two classes. For example, if we're classifing whether an email is spam, we can use 0 for \"no\" and 1 for \"yes\"."
      ],
      "metadata": {
        "id": "0XFQ1FIlCTtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Iris dataset."
      ],
      "metadata": {
        "id": "tu0937tM3RUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 50 samples of 3 different species of iris (150 samples total)\n",
        "- measurements: sepal length, sepal width, petal length, petal width\n",
        "- More information in the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Iris)"
      ],
      "metadata": {
        "id": "UwNnifc53jMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Constants"
      ],
      "metadata": {
        "id": "DTV4ACj8ystI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i38aJANw3FM7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FIG_SIZE = (12, 10)\n",
        "plt.rc('figure', figsize=FIG_SIZE)"
      ],
      "metadata": {
        "id": "Gktxmraynubp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data"
      ],
      "metadata": {
        "id": "H4rWSR4VBTd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "iris"
      ],
      "metadata": {
        "id": "SwT5C0Fa4oLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(iris)"
      ],
      "metadata": {
        "id": "kiYTHVLx4tWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.data"
      ],
      "metadata": {
        "id": "E9fMZBCo48Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FqX-y4M40XM"
      },
      "outputs": [],
      "source": [
        "# print the names of the four features\n",
        "iris.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8lx8WtR40XN"
      },
      "outputs": [],
      "source": [
        "# print integers representing the species of each observation\n",
        "iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjo8qQVD40XN"
      },
      "outputs": [],
      "source": [
        "# print the encoding scheme for species: 0 = setosa, 1 = versicolor, 2 = virginica\n",
        "iris.target_names.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The rules of Machine Learning\n",
        "\n",
        "\n",
        "1.   Features and labels are **separate objects**.\n",
        "2.   Features are **numeric**, and if the problem is a regression problem the labels are also **numeric**.\n",
        "3.   Features and labels are **NumPy arrays**.\n",
        "4.   Features and label have **specific shapes**. The features must have **two** dimensions: the first one represents the number of samples/observations and the second - the number of features. The label must have one dimension which is the number of samples (equal to the first dimension of the feature object).\n",
        "5.   The feature object is named `X` by convention. `X` is capitalized because it represents a matrix.\n",
        "6.   The label object is named `y` by convention. `y` is lowercase because it represents a vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "7pLuTZwY6ABb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "qeYefhouukB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a dataframe\n",
        "\n",
        "# Step 1: Add the features\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df"
      ],
      "metadata": {
        "id": "a3UrzM-nmuGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Add target\n",
        "df['Target'] = iris.target\n",
        "df"
      ],
      "metadata": {
        "id": "Sw-cH9eDm-hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Everything is a number\n",
        "df.info()"
      ],
      "metadata": {
        "id": "IpyPEbqtnONr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No class imbalance!\n",
        "df['Target'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "PgukyrguvYZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are no missing values\n",
        "df.isna().mean()"
      ],
      "metadata": {
        "id": "LYnVc6ibnQ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The sepal width feature has very low variance. Probably won't help in\n",
        "# predicting the target.\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "wsUkN7tZzSYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the correlation\n",
        "df.corr()\n",
        "# Everything is very correlated with the target"
      ],
      "metadata": {
        "id": "RY3OrcROnbcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(df['sepal width (cm)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IL1sNP3My4kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(df['petal length (cm)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r4-pZ7kXz8b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Using petal length and petal width to predict target')\n",
        "plt.xlabel('petal width')\n",
        "plt.ylabel('petal length')\n",
        "\n",
        "scatter=plt.scatter(df['petal width (cm)'], df['petal length (cm)'], c=df['Target'])\n",
        "\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=iris.target_names.tolist())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g9uGu5bAnUbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "ckBmbrp0uuFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inputs(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  # Split into X and y\n",
        "  y = df['Target']\n",
        "  X = df.drop(['Target'], axis=1)\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "Mpx-pC3Euvll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocess_inputs(df)"
      ],
      "metadata": {
        "id": "wd5X35jm7uzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "jRQD8s4Mu-FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "WKE2ETlfu_SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a model\n",
        "\n",
        "For classification tasks, the most widely used models include:\n",
        "\n",
        "- KNN\n",
        "- LogisticRegression\n",
        "- Tree Based (Decision Tree, Random Forest, Adaboost, etc)\n",
        "- Perceptron\n",
        "- SVM\n",
        "- Naive Bayes (for text classification)\n",
        "- Ensemble (combination of the above)\n",
        "\n",
        "Today, we'll look at KNN and LogisticRegression. Do you remember how KNN worked?"
      ],
      "metadata": {
        "id": "1cNHF-6ZwqZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The K-nearest neighbors (KNN) algorithm\n",
        "\n",
        "1. Pick a value for K.\n",
        "2. Search for the K observations in the training data that are \"nearest\" to the measurements of the unknown iris.\n",
        "3. Use the most popular response value from the K nearest neighbors as the predicted response value for the unknown iris."
      ],
      "metadata": {
        "id": "9jYeejSq0EaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "class KNN:\n",
        "  def __init__(self, k=3):\n",
        "    self.k = k\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.X_train = X\n",
        "    self.y_train = y\n",
        "\n",
        "  def predict(self, X):\n",
        "    y_pred = np.array([self._predict(x) for x in X])\n",
        "    return y_pred\n",
        "\n",
        "  def _predict(self, x):\n",
        "    # Compute distances between x and all examples in the training set\n",
        "    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "    # Sort by distance and return indices of the first k neighbors\n",
        "    k_idx = np.argsort(distances)[:self.k]\n",
        "\n",
        "    # Extract the labels of the k nearest neighbor training samples\n",
        "    k_neighbor_labels = self.y_train[k_idx]\n",
        "    \n",
        "    # return the most common class label\n",
        "    most_common = Counter(k_neighbor_labels).most_common(1)\n",
        "    \n",
        "    return most_common[0][0]"
      ],
      "metadata": {
        "id": "BNJQWpOFG5WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(\n",
        "    n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2\n",
        ")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-pzwBe8VI4Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123\n",
        ")\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ah8pHPwDJB5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNN(k=2)\n",
        "knn.fit(X_train, y_train)\n",
        "predictions = knn.predict(X_test)\n",
        "\n",
        "print(\"Training classification accuracy\", metrics.accuracy_score(y_train, knn.predict(X_train)))\n",
        "print(\"Testing classification accuracy\", metrics.accuracy_score(y_test, predictions))\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZD0mkqjoHtlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Logistic Regression algorithm"
      ],
      "metadata": {
        "id": "hCTjDq0XG1cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic_Regression():\n",
        "  def __init__(self, learning_rate, no_of_iterations):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    self.m, self.n = X.shape\n",
        "\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "  def update_weights(self):\n",
        "    # Y_hat formula (sigmoid function)\n",
        "    Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))    \n",
        "\n",
        "    dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
        "    db = (1/self.m)*np.sum(Y_hat - self.Y)\n",
        "\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "  def predict(self, X):\n",
        "    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) )) \n",
        "    Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
        "    return Y_pred"
      ],
      "metadata": {
        "id": "8xfrOsoJGsC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = Logistic_Regression(learning_rate=0.3, no_of_iterations=200)\n",
        "logreg.fit(X_train, y_train)\n",
        "predictions = logreg.predict(X_test)\n",
        "\n",
        "print(\"Training classification accuracy\", metrics.accuracy_score(y_train, logreg.predict(X_train)))\n",
        "print(\"Testing classification accuracy\", metrics.accuracy_score(y_test, predictions))\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJdqIANarOGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scikit-learn 5-step modeling pattern"
      ],
      "metadata": {
        "id": "enbKh3ak2SUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocess_inputs(df)"
      ],
      "metadata": {
        "id": "xyb5blBqeo9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Import the class you plan to use"
      ],
      "metadata": {
        "id": "PA625b0n2UK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** \"Instantiate\" the \"estimator\"\n",
        "\n",
        "- \"Estimator\" is scikit-learn's term for model"
      ],
      "metadata": {
        "id": "mxuU05mb2VUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(n_neighbors=1)"
      ],
      "metadata": {
        "id": "uieFWtLX2X71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Name of the object does not matter\n",
        "- Can specify tuning parameters (aka \"hyperparameters\") during this step\n",
        "- All parameters not specified are set to their defaults"
      ],
      "metadata": {
        "id": "2u6dkYL42W_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Fit the model with data (aka \"model training\")\n",
        "\n",
        "- Model is learning the relationship between X and y\n",
        "- Occurs in-place"
      ],
      "metadata": {
        "id": "ETjmows52ZlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first train the model on the whole dataset.\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "4bCZsoV-2agP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Predict the response for a new observation\n",
        "\n",
        "- New observations are called \"out-of-sample\" data\n",
        "- Uses the information it learned during the model training process\n",
        "- **NOTE**: Only **2D** data arrays can be passed to models when making a prediction. They should have the same feature names as the dataframe that was used in the `.fit` method."
      ],
      "metadata": {
        "id": "-hIiUJdx2csx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.DataFrame([[5.5, 3, 1.5, 0]], columns=iris.feature_names)\n",
        "X_test"
      ],
      "metadata": {
        "id": "Y4rvIK9m4-AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a fake example and see what the model will output.\n",
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "lsKpdWGo2dG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Returns a NumPy array\n",
        "- Can predict for multiple observations at once"
      ],
      "metadata": {
        "id": "KfSMCtLF2els"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:** Evaluate the model using a metric. **EVALUATION MUST BE DONE ON A VALIDATION / TEST SET**. That way, we know whether our model can truly generalize. It's done on training set only for comparison with the results from the validation / test set.\n",
        "\n",
        "- **accuracy**: if there is no class imbalance could suffice (classification)\n",
        "- **f1 score**: if class imbalance, this is definitely a must\n",
        "- **classification report**: Text summary of the precision, recall, F1 score for each class and overall accuracy .\n",
        "- **confusion matrix**: shows how often the model predict each class"
      ],
      "metadata": {
        "id": "UEmHE5wR2k8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that this doesn't tell us much, since we're\n",
        "# evaluating on the training set\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "Zd-b4rFJ3U6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X)\n",
        "print(classification_report(y, y_pred))"
      ],
      "metadata": {
        "id": "1wNjly756EA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ],
      "metadata": {
        "id": "BStNKRJN6fhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "QfRWjacR6dOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X)\n",
        "print(classification_report(y, y_pred))"
      ],
      "metadata": {
        "id": "cwCngz4x61Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "8HzLOwD_6vzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a different model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "cx9blRhL66-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the model failed to converge.\n",
        "# Let's try to increase the iterations.\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "6iBvJrUa7nBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X)\n",
        "print(classification_report(y, y_pred))"
      ],
      "metadata": {
        "id": "aOljowfR_1Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "Xn6iK52S8I7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation procedure #1: Train and test on the entire dataset\n",
        "\n",
        "1. Train the model on the **entire dataset**.\n",
        "2. Test the model on the **same dataset**, and evaluate how well we did by comparing the **predicted** response values with the **true** response values.\n",
        "\n",
        "We applied used in the previous notebook."
      ],
      "metadata": {
        "id": "niOD9sZZAvvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression"
      ],
      "metadata": {
        "id": "Ui44yW5WCD9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFWvBn9AAJdy"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "len(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification accuracy:\n",
        "\n",
        "- **Proportion** of correct predictions\n",
        "- Common **evaluation metric** for classification problems"
      ],
      "metadata": {
        "id": "5DZHQV77CiJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "JJVIu142CfAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative way.\n",
        "metrics.accuracy_score(y, y_pred)"
      ],
      "metadata": {
        "id": "rIQMbrsRCrip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Known as **training accuracy** when you train and test the model on the same data"
      ],
      "metadata": {
        "id": "yTDKfzI7DA1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN (K=5)"
      ],
      "metadata": {
        "id": "UyJKHn0fDHWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(5)\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "rSlgg9UDC1Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN (K=1)"
      ],
      "metadata": {
        "id": "FfcaP6ArDZMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(1)\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "-pKlukOiDV5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN (K=1) will always have 100% training accuracy, since to make a prediction for any observation in the training set, KNN would search for the nearest observation in the training set and would find that exact same observation! In other words, KNN has memorized the training set and because we're using the exact same data, it'll always make correct predictions."
      ],
      "metadata": {
        "id": "mOB8s3P4DoEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems with training and testing on the same data\n",
        "\n",
        "- Goal is to estimate likely performance of a model on **out-of-sample data**\n",
        "- But, maximizing training accuracy rewards **overly complex models** that won't necessarily generalize\n",
        "- Unnecessarily complex models **overfit** the training data"
      ],
      "metadata": {
        "id": "nTClt9I3Epb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Rule of thumb**: For KNN a lower value of K creates a more complex model."
      ],
      "metadata": {
        "id": "SVwf9CtWE73l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation procedure #2: Train/test split\n",
        "\n",
        "1. Split the dataset into two pieces: a **training set** and a **testing set**.\n",
        "2. Train the model on the **training set**.\n",
        "3. Test the model on the **testing set**, and evaluate how well we did."
      ],
      "metadata": {
        "id": "x-pv4cZaFSH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ex6GSnNpE7hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inputs(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  # Split into X and y\n",
        "  y = df['Target']\n",
        "  X = df.drop(['Target'], axis=1)\n",
        "\n",
        "  # Train/test split\n",
        "  # usually test_size is between 0.10 and 0.40\n",
        "  # use random state to get same results on every run\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "hJoIhUboDa9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = preprocess_inputs(df)"
      ],
      "metadata": {
        "id": "e4Ln-W4fGC4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "XWF994UYGtAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "FJ9_BmCmGySq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "1FodVbxLG1ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "0QTGdlOYG4HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What did this accomplish?\n",
        "\n",
        "- Model can be trained and tested on **different data**\n",
        "- Response values are known for the testing set, and thus **predictions can be evaluated**\n",
        "- **Testing accuracy** is a better estimate than training accuracy of out-of-sample performance"
      ],
      "metadata": {
        "id": "PPt-kxaxJUQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "EnSYJw9ZHGCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN (K=5)\n",
        "model = KNeighborsClassifier(5)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "19XkGJf0HNuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN (K=1)\n",
        "model = KNeighborsClassifier(1)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "ACKOUBbKHecc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Conclusion**: Logistic regression is likely the best model for this split."
      ],
      "metadata": {
        "id": "VM76xHiNHwMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can we find the best value for k?"
      ],
      "metadata": {
        "id": "3m9M6pvVH4eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "ks = range(1, 31)\n",
        "\n",
        "for k in ks:\n",
        "  model = KNeighborsClassifier(k)\n",
        "  model.fit(X_train, y_train)\n",
        "  scores.append(model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "88ejxE9DHfrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Searching for the best value of KNN')\n",
        "plt.xlabel('Value for K')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.plot(ks, scores)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v93gYetFIInV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Training accuracy** rises as model complexity increases\n",
        "- **Testing accuracy** penalizes models that are too complex or not complex enough\n",
        "- For KNN models, complexity is determined by the **value of K** (lower value = more complex)"
      ],
      "metadata": {
        "id": "tvYwHxDjJeeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN (K=15)\n",
        "model = KNeighborsClassifier(15)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "IEXa0LbAJeOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems with train/test split\n",
        "\n",
        "- Provides a **high-variance estimate** of out-of-sample accuracy\n",
        "- **K-fold cross-validation** overcomes this limitation. We'll see it next time.\n",
        "- But, train/test split is still useful because of its **flexibility and speed**"
      ],
      "metadata": {
        "id": "knotj-RHJxel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For home\n",
        "\n",
        "Do classification on the Titanic dataset."
      ],
      "metadata": {
        "id": "D6pAtGWUAhWE"
      }
    }
  ]
}